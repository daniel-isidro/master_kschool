# Deep Learning
## Juan Ar√©valo, Cepsa

* Deep learning

  W = weight matrix, on every layer it evolves and improves

  Lineal transformations are equivalent to vector rotations. To make non-linear transformations, we use activation functions
  
  Most used activation function is * rectifier linear unit * or relu
  
  Relu returns 0 if input is negative, and returns x if input is positive
  
  We decide how many layers have the model, but the weight matrices are selected automatically
  
  Every layer decides what parts of its input are relevant or not, to resolve the problem
  
  Reinforced learning: At the end of the process we tell the model whether the output was correct or not
  
  Neurons are the components of the output y. b and y have the same size
  
  Every layer changes the size of the input. A neural network is a series of layers
  
* Training process: backpropragation algorythm, specifically gradient descent algorythms
